# src/data/fetch_datasets.py
import os
import sys
import time
import requests
import pandas as pd
import subprocess
from tqdm import tqdm  # üí° CORRECCI√ìN CR√çTICA: Importar la clase tqdm directamente

# --- CONFIGURACI√ìN DE RUTAS PORTABLE (3 NIVELES) ---\n# Sube tres niveles: src/data -> src -> anime_recommender (ROOT_DIR)
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# Construye la ruta a la carpeta 'data' en la ra√≠z
DATA_DIR = os.path.join(ROOT_DIR, "data")
MERGED_PATH = os.path.join(DATA_DIR, "merged_anime.csv")

# PREPARE_SCRIPT_PATH no es necesario en este script
# PREPARE_SCRIPT_PATH = os.path.join(os.path.dirname(__file__), "prepare_data.py") 

ANILIST_API = "https://graphql.anilist.co"
QUERY = """
query ($page: Int, $perPage: Int) {
  Page(page: $page, perPage: $perPage) {
    media(type: ANIME, sort: POPULARITY_DESC) {
      id # AniList ID
      idMal # MyAnimeList ID (CR√çTICO para la fusi√≥n)
      title {
        romaji
        english
        native
      }
      description(asHtml: false)
      genres
      tags {
        name
      }
      averageScore
      episodes
      status
      type
      siteUrl
      studios(isMain: true) {
        nodes {
          name
        }
      }
    }
  }
}
"""

def fetch_page(page, per_page=50):
    """Obtiene una p√°gina de la API de AniList."""
    variables = {'page': page, 'perPage': per_page}
    
    try:
        response = requests.post(ANILIST_API, json={'query': QUERY, 'variables': variables})
        response.raise_for_status()
        data = response.json()
        
        # Verificar si hay datos en la p√°gina
        if data and 'data' in data and data['data']['Page']['media']:
            return data['data']['Page']['media']
        return None
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de red al obtener la p√°gina {page}: {e}", file=sys.stderr)
        return None

def normalize(media_list):
    """Normaliza y limpia la lista de medios a un DataFrame de Pandas."""
    rows = []
    for m in media_list:
        title = m.get('title', {})
        mal_id = m.get('idMal')
        
        # Saltarse si no tiene MAL ID, ya que es la clave de fusi√≥n
        if mal_id is None:
            continue
            
        # Asegurar que los campos de lista sean listas
        tags = [t['name'] for t in m.get('tags', []) if isinstance(t, dict) and t.get('name')]
        genres = m.get('genres', [])
        
        # Normalizar la puntuaci√≥n de 1-100 a 1-10.0
        score_100 = m.get('averageScore', 0)
        score_10 = round(score_100 / 10, 1) if score_100 else 0.0

        rows.append({
            "id": m.get("id"), # AniList ID
            "idMal": mal_id, # MAL ID
            "title": title.get('romaji') or title.get('english') or title.get('native') or "T√≠tulo Desconocido",
            "description": m.get("description"),
            "genres": genres,
            "tags": tags,
            "averageScore": score_10, # Puntuaci√≥n normalizada
            "episodes": m.get("episodes"),
            "status": m.get("status"),
            "type": m.get("type"),
            "siteUrl": m.get("siteUrl"),
            "studios": [
                n["name"]
                for n in (m.get("studios", {}).get("nodes", []) or [])
                if isinstance(n, dict) and n.get("name")
            ],
        })

    df = pd.DataFrame(rows)
    # CR√çTICO: Eliminar duplicados bas√°ndose en el ID de AniList
    df = df.drop_duplicates(subset=["id"]).reset_index(drop=True) 
    # CR√çTICO: Solo mantener animes que tengan un MalID para la fusi√≥n
    df = df[df["idMal"].notna()].copy() 

    return df

def fetch_all(max_pages=50):
    all_data = []
    
    # üí° CORRECCI√ìN CR√çTICA: Usar la clase tqdm para la barra de progreso
    with tqdm(total=max_pages * 50, desc="Descargando Animes", unit="item", dynamic_ncols=True, leave=True) as pbar:
        for page in range(1, max_pages + 1):
            try:
                media = fetch_page(page)
                if media is None:
                    break
                
                all_data.extend(media)
                pbar.update(len(media))
                
                time.sleep(1) # Respetar el l√≠mite de rate de la API de AniList
            except Exception as e:
                print(f"‚ùå Error inesperado en la p√°gina {page}: {e}", file=sys.stderr)
                break
            
    return normalize(all_data)

def main():
    df = fetch_all()
    if df.empty:
        print("‚ùå Fallo al descargar el dataset base.", file=sys.stderr)
        sys.exit(1)
        
    # Asegurarse de que el directorio 'data' exista en la ra√≠z del proyecto
    os.makedirs(DATA_DIR, exist_ok=True) 
    df.to_csv(MERGED_PATH, index=False)
    
    print(f"üéâ Dataset base de {len(df)} animes guardado en: {MERGED_PATH}", file=sys.stderr)


if __name__ == '__main__':
    main()